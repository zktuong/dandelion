{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e872038e",
   "metadata": {},
   "source": [
    "# Pre-processing (Re-annotation)\n",
    "\n",
    "## Foreword\n",
    "\n",
    "**dandelion** is written in `python 3` and is a single-cell BCR/TCR V(D)J-seq analysis package. It borrows some tools from the fantastic [immcantation suite](https://immcantation.readthedocs.io/) [[Gupta2015]](https://academic.oup.com/bioinformatics/article/31/20/3356/195677), implementing a workflow to streamline the pre-processing and exploratory stages for analyzing single-cell BCR/TCR V(D)J-seq data from 10X Genomics. Post-processed data from **dandelion** can be smoothly transferred to [Scanpy](https://scanpy.readthedocs.io/)/[AnnData](https://anndata.readthedocs.io/) [[Wolf2018]](https://doi.org/10.1186/s13059-017-1382-0) object for integration and exploration of V(D)J-seq data and RNA-seq data. I hope to be able to introduce some new single-cell V(D)J-seq exploratory tools down the road through *dandelion*. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "    \n",
    "This section will cover the initial pre-processing of files after 10X's `CellRanger vdj` immune profiling data analysis (BCR) pipeline **manually**. As mentioned, there is now a [singularity container](https://sc-dandelion.readthedocs.io/en/latest/notebooks/Q1-singularity-preprocessing.html) that can automate the steps outlined below.\n",
    "\n",
    "</div>\n",
    "    \n",
    "We will download the 10X data sets to process for this tutorial:\n",
    "```bash\n",
    "# create sub-folders\n",
    "mkdir -p dandelion_tutorial/vdj_nextgem_hs_pbmc3\n",
    "mkdir -p dandelion_tutorial/vdj_v1_hs_pbmc3\n",
    "mkdir -p dandelion_tutorial/sc5p_v2_hs_PBMC_10k\n",
    "mkdir -p dandelion_tutorial/sc5p_v2_hs_PBMC_1k\n",
    "\n",
    "# change into each directory and download the necessary files\n",
    "cd dandelion_tutorial/vdj_v1_hs_pbmc3;\n",
    "wget -O filtered_feature_bc_matrix.h5 https://cf.10xgenomics.com/samples/cell-vdj/3.1.0/vdj_v1_hs_pbmc3/vdj_v1_hs_pbmc3_filtered_feature_bc_matrix.h5;\n",
    "wget -O filtered_contig_annotations.csv https://cf.10xgenomics.com/samples/cell-vdj/3.1.0/vdj_v1_hs_pbmc3/vdj_v1_hs_pbmc3_b_filtered_contig_annotations.csv;\n",
    "wget -O filtered_contig.fasta https://cf.10xgenomics.com/samples/cell-vdj/3.1.0/vdj_v1_hs_pbmc3/vdj_v1_hs_pbmc3_b_filtered_contig.fasta;\n",
    "\n",
    "cd ../vdj_nextgem_hs_pbmc3\n",
    "wget -O filtered_feature_bc_matrix.h5 https://cf.10xgenomics.com/samples/cell-vdj/3.1.0/vdj_nextgem_hs_pbmc3/vdj_nextgem_hs_pbmc3_filtered_feature_bc_matrix.h5;\n",
    "wget -O filtered_contig_annotations.csv https://cf.10xgenomics.com/samples/cell-vdj/3.1.0/vdj_nextgem_hs_pbmc3/vdj_nextgem_hs_pbmc3_b_filtered_contig_annotations.csv;\n",
    "wget -O filtered_contig.fasta https://cf.10xgenomics.com/samples/cell-vdj/3.1.0/vdj_nextgem_hs_pbmc3/vdj_nextgem_hs_pbmc3_b_filtered_contig.fasta;\n",
    "\n",
    "cd ../sc5p_v2_hs_PBMC_10k;\n",
    "wget -O filtered_feature_bc_matrix.h5 https://cf.10xgenomics.com/samples/cell-vdj/4.0.0/sc5p_v2_hs_PBMC_10k/sc5p_v2_hs_PBMC_10k_filtered_feature_bc_matrix.h5;\n",
    "wget -O filtered_contig_annotations.csv https://cf.10xgenomics.com/samples/cell-vdj/4.0.0/sc5p_v2_hs_PBMC_10k/sc5p_v2_hs_PBMC_10k_b_filtered_contig_annotations.csv;\n",
    "wget -O filtered_contig.fasta https://cf.10xgenomics.com/samples/cell-vdj/4.0.0/sc5p_v2_hs_PBMC_10k/sc5p_v2_hs_PBMC_10k_b_filtered_contig.fasta;\n",
    "\n",
    "cd ../sc5p_v2_hs_PBMC_1k;\n",
    "wget -O filtered_feature_bc_matrix.h5 wget https://cf.10xgenomics.com/samples/cell-vdj/4.0.0/sc5p_v2_hs_PBMC_1k/sc5p_v2_hs_PBMC_1k_filtered_feature_bc_matrix.h5;\n",
    "wget -O filtered_contig_annotations.csv wget https://cf.10xgenomics.com/samples/cell-vdj/4.0.0/sc5p_v2_hs_PBMC_1k/sc5p_v2_hs_PBMC_1k_b_filtered_contig_annotations.csv;\n",
    "wget -O filtered_contig.fasta https://cf.10xgenomics.com/samples/cell-vdj/4.0.0/sc5p_v2_hs_PBMC_1k/sc5p_v2_hs_PBMC_1k_b_filtered_contig.fasta;\n",
    "```\n",
    "\n",
    "`Dandelion`'s reannotation workflow requires `CellRanger` formatted-files to start, particularly either `all_contig.fasta` or `filtered_contig.fasta` and corresponding `all_contig_annotations.csv` and `filtered_contig_annotations.csv`. If you only have an `AIRR` rearrangement file, you can still load it into `dandelion` and write the files out with `.write_10x` function later.\n",
    "\n",
    "I'm running everything with the `filtered` files to speed up the tutorial but I would recommend starting with the `all` files as they contain more information, and it is easier to filter the contigs than to try and add them back in later.. I'm using a standard laptop for the analysis here: <strike>entry level 2017 MacBook Pro with 2.3 GHz Intel Core i5 processor and 16 GB 2133 MHz LPDDR3 ram.</strike> updated to 2022 Macbook Pro M2 chip with 8 core CPU and 24 GB ram.\n",
    "\n",
    "If you followed the installation instructions, you should have the requisite auxillary softwares installed already. Otherwise, you can download them manually: [blast+](https://ftp.ncbi.nih.gov/blast/executables/igblast/release/LATEST/) and [igblast](https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/).\n",
    "\n",
    "For convenience, in **shell**, export the path to the database folders like as follows:\n",
    "```bash\n",
    "# bash/shell\n",
    "echo \"export GERMLINE=$HOME/Documents/Github/dandelion/container/database/germlines/\" >> ~/.bash_profile\n",
    "echo \"export IGDATA=$HOME/Documents/Github/dandelion/container/database/igblast/\" >> ~/.bash_profile\n",
    "echo \"export BLASTDB=$HOME/Documents/Github/dandelion/container/database/blast/\" >> ~/.bash_profile\n",
    "# reload\n",
    "source ~/.bash_profile\n",
    "```\n",
    "The databases for igblast are basically setup using [changeo's instructions](https://changeo.readthedocs.io/en/stable/examples/igblast.html). \n",
    "\n",
    "If you are using a jupyter notebook initialized via jupyterhub instance, you might want to try the fix to a known issue where pathing requires some adjustments https://github.com/zktuong/dandelion/discussions/146.\n",
    "\n",
    "For reannotation of constant genes, reference fasta files were downloaded from IMGT and only sequences corresponding to *CH1* region for each constant gene/allele were retained. The headers were trimmed to only keep the gene and allele information. Links to find the sequences can be found here : [human](http://www.imgt.org/genedb/GENElect?query=7.2+IGHC&species=Homo+sapiens) and [mouse](http://www.imgt.org/genedb/GENElect?query=7.2+IGHC&species=Mus).\n",
    "\n",
    "The utility function `ddl.utl.makeblastdb` is a wrapper for:\n",
    "\n",
    "```bash\n",
    "# bash/shell\n",
    "makeblastdb -dbtype nucl -parse_seqids -in $BLASTDB/human/human_BCR_C.fasta\n",
    "```\n",
    "\n",
    "This does the same thing:\n",
    "```python\n",
    "# python\n",
    "ddl.utl.makeblastdb(os.path.expanduser(\"~/Documents/Github/dandelion/container/database/blast/human/human_BCR_C.fasta\")\n",
    "```\n",
    "\n",
    "This section will now demonstrate how I batch process multiple samples/files from the same donor, as it will become important later on.\n",
    "\n",
    "Finally, because you are running the preprocessing manually, you will need to install some R packages so that they work later:\n",
    "\n",
    "```R\n",
    "install.packages(c(\"optparse\", \"airr\", \"shazam\", \"alakazam\", \"tigger\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import dandelion as ddl\n",
    "\n",
    "ddl.logging.print_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to somewhere more workable\n",
    "os.chdir(os.path.expanduser(\"~/Downloads/dandelion_tutorial/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bffc3c",
   "metadata": {},
   "source": [
    "## Step 1: Formatting the headers of the Cell Ranger fasta file\n",
    "Here, I'm adding a prefix to the headers of each contig in the fasta files, via the function `pp.format_fastas`. The prefix is basically just the folder name, so in this case it's:\n",
    "`sc5p_v2_hs_PBMC_1k`, `sc5p_v2_hs_PBMC_10k`, `vdj_v1_hs_pbmc3` and `vdj_nextgem_hs_pbmc3`.\n",
    "\n",
    "The function will also create sub-folders where a new fasta file and all subsequent files will be located. The function will also add the prefix to the corresponding annotation file automatically and create a copy in the same folder as the formatted fasta file. \n",
    "\n",
    "This is to ensure that the barcodes are consistent throughout so as not to interfere with subsequent integration with the gene expression data, which will be covered in subsequent sections. \n",
    "\n",
    "The file structure should look something like this later on if the settings are left as default. The tmp directory can be deleted once the initial preprocessing has completed.\n",
    "```console\n",
    "sc5p_v2_hs_PBMC_1k\n",
    "├── dandelion\n",
    "│   ├── filtered_contig.fasta\n",
    "│   ├── filtered_contig_annotations.csv\n",
    "│   ├── filtered_contig_igblast_db-pass_genotyped.tsv\n",
    "│   └── tmp\n",
    "│       ├── filtered_contig_igblast.fmt7\n",
    "│       ├── filtered_contig_igblast.tsv\n",
    "│       ├── filtered_contig_igblast_db-pass.blastsummary.txt\n",
    "│       ├── filtered_contig_igblast_db-pass.tsv\n",
    "│       ├── filtered_contig_igblast_db-pass.xml\n",
    "│       ├── filtered_contig_igblast_db-pass_genotyped.tsv\n",
    "│       ├── filtered_contig_igblast_db-pass_heavy_parse-select.tsv\n",
    "│       └── filtered_contig_igblast_db-pass_light_parse-select.tsv\n",
    "├── filtered_contig.fasta\n",
    "├── filtered_contig_annotations.csv\n",
    "└── filtered_feature_bc_matrix.h5\n",
    "```\n",
    "\n",
    "The first option of `pp.format_fastas` accepts a list of the fasta file paths to reformat, or list of names of folders containing the fasta files; each folder should only contain 1 fasta file, and 1 contig_annotation.csv. Make sure there's no hidden files and delete those if present.\n",
    "\n",
    "You can provide `prefixes` and/or `suffixes` to add the the cell/contig barcodes as a list and they will be formatted accordingly. The prefixes/suffixes will be separated by an underscore (`_`) if left as default but that can be adjusted with the `sep` option. \n",
    "\n",
    "If you choose not to provide a prefix/suffix, then the function will simply make a copy of the original files and place it in the `dandelion` sub-folders.\n",
    "\n",
    "For more complex experimental setups, such as with data from multiplexed experiments, please contact me (z.tuong@uq.edu.au) and I can walk you through a slightly more advanced set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first option is a list of fasta files to format and the second option is the list of prefix to add to each file.\n",
    "samples = [\n",
    "    \"sc5p_v2_hs_PBMC_1k\",\n",
    "    \"sc5p_v2_hs_PBMC_10k\",\n",
    "    \"vdj_v1_hs_pbmc3\",\n",
    "    \"vdj_nextgem_hs_pbmc3\",\n",
    "]\n",
    "# because I'm using the 'filtered' files, i need to specify the filename_prefix\n",
    "ddl.pp.format_fastas(samples, prefix=samples, filename_prefix=\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6b44f",
   "metadata": {},
   "source": [
    "Please specify the `filename_prefix` option for all preprocessing functions below (except for `quantify_mutations`).\n",
    "\n",
    "For example, use `filename_prefix=\"all\"` for `all_contig.fasta`, or `filename_prefix=<insertprefix>` for any files that are named `<insertprefix>_contig.fasta`. \n",
    "\n",
    "If you are running more than 1 sample and if each filename prefix needs to be specified, `filename_prefix` in `ddl.pp.format_fastas` will accept a list of prefixes i.e. `filename_prefix=['all', 'filtered', ...]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf28d7e",
   "metadata": {},
   "source": [
    "## Step 2: Reannotate the V/D/J genes with *igblastn*\n",
    "\n",
    "Like immcantation, we will reannotate the V(D)J genes with igblastn using the latest IMGT reference databases. Dandelion's `pp.reannotate_genes` will use `flavour=\"strict\"` to run `igblastn`, imposing lower e-value and higher D-penalty cut offs. The original behaviour i.e. with [changeo](https://changeo.readthedocs.io/en/stable/examples/10x.html)'s `AssignGenes.py`, is toggled with `flavour=\"original\"`. Additionally, there is now an additional `assign_dj` option (default is `True`), which will use blastn to assign a stricter call for the D and J genes because [igblastn can return random assignments if it cannot detect a V gene](https://www.ncbi.nlm.nih.gov/igblast/faq.html). In the `tmp` folder, there will also be a table where all alignments generated in this step will be shown. All the column headers are now adhering to the [AIRR](http://docs.airr-community.org/) standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl.pp.reannotate_genes(samples, filename_prefix=\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b82a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "If you did not set a path to the igblast or germline paths in the environment above, you need to specify the path to the folders containing the fasta files directly.\n",
    "\n",
    "```python\n",
    "ddl.pp.reannotate_genes(samples, \n",
    "    igblast_db=\"database/igblast/\",\n",
    "    germline=\"database/germlines/imgt/human/vdj/\")\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ec016",
   "metadata": {},
   "source": [
    "## Step 3 : Reassigning heavy chain V gene alleles *(optional but recommended)*\n",
    "\n",
    "Next, we use `immcantation`'s `TIgGER` [[Gadala-Maria2015]](https://www.pnas.org/content/112/8/E862) method to reassign allelic calls for heavy chain V genes with `pp.reassign_alleles`. As stated in TIgGER's [website](https://tigger.readthedocs.io/en/stable/) and [manuscript](https://www.pnas.org/content/112/8/E862), *'TIgGER is a computational method that significantly improves V(D)J allele assignments by first determining the complete set of gene segments carried by an individual (including novel alleles) from V(D)J-rearrange sequences. TIgGER can then infer a subject’s genotype from these sequences, and use this genotype to correct the initial V(D)J allele assignments.'*\n",
    "\n",
    "This impacts on how contigs are chosen for finding clones later. It is also important when considering to do mutational analysis. For convenience, germline sequences are reconstructed at this step using the corrected V-gene alleles. Therefore, it is highly recommended to run it. \n",
    "\n",
    "However, this will only work properly if there is sufficient contigs. An ideal scenario would be to run it on multiple samples from the same subject to allow for more information to be used to confidently assign a genotyped *v_call*. In this tutorial, I'm assuming the four samples can be split into two sets where sets of two corresponds to a different/single individual. So while important, this step can be skipped if you don't have enough data to do this. \n",
    "\n",
    "`pp.reassign_alleles` requires the `combined_folder` option to be specified so that a merged/concatenated file can be produced for running TIgGER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0432c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassigning alleles on the first set of samples\n",
    "ddl.pp.reassign_alleles(\n",
    "    samples[:2], combined_folder=\"tutorial_scgp1\", filename_prefix=\"filtered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760adb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassigning alleles on the second set of samples\n",
    "ddl.pp.reassign_alleles(\n",
    "    samples[2:], combined_folder=\"tutorial_scgp2\", filename_prefix=\"filtered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7587731",
   "metadata": {},
   "source": [
    "We can see that most of the original ambiguous V calls have now been corrected and only a few remain.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "Note\n",
    "    \n",
    "Similar to above, if you you can specify the path to the folder containing the fasta files accordingly:\n",
    "\n",
    "```python\n",
    "ddl.pp.reassign_alleles(samples[2:], combined_folder='tutorial_scgp2', germline=\"database/germlines/imgt/human/vdj\", filename_prefix=\"filtered\")\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c546938",
   "metadata": {},
   "source": [
    "## Step 4: Assigning constant region calls\n",
    "\n",
    "Cell Ranger's annotation files provides a *c_gene* column, but rather than simply relying on Cell Ranger's annotation, it is common to use *[immcantation-presto's MaskPrimers.py](https://presto.readthedocs.io/en/version-0.5.3---license-change/tools/MaskPrimers.html)* with a custom primer list. \n",
    "\n",
    "As an alternative, `dandelion` includes a pre-processing function, `pp.assign_isotypes`, to use *blastn* to annotate constant region calls for all contigs and retrieves the call, merging it with the tsv files. This function will overwrite the output from previous steps and add a *c_call* column at the end, or replace the existing column if it already exists. The Cell Ranger calls are returned as `c_call_10x`.\n",
    "\n",
    "Further, to deal with incorrect constant gene calls due to insufficient length, a pairwise alignment will be run against [curated sequences](https://immunology.sciencemag.org/content/6/56/eabe6291) that were deemed to be highly specific in distinguishing `IGHA1` vs `IGHA2`, and `IGHG1` to `IGHG4`. I have also curated sets of sequences that should help deal with `IGLC3/6/7` as these are problematic too. If there is insufficient info, the `c_call` will be returned as a combination of the most aligned sets of sequences. Because of how similar the lambda light chains are, extremely ambiguous calls (only able to map to a common sequence across the light chains) will be returned as `IGLC`. This typically occurs when the constant sequence is very short. Those that have equal alignment scores between `IGLC3/6/7` sequences and the common sequence will be returned as a concatenated call; for example, a contig initially annotated as `IGLC3` will be returned as `IGLC,IGLC3`. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "\n",
    "The curated sequences can be updated/replaced with a dict-of-dict-of-dict style dictionary via the option `correction_dict`. The provided dictionary should be a nested dictionary like the following:\n",
    "```python\n",
    "primer_dict = {\n",
    "    'IGHG':{\n",
    "        'IGHG1':'GCCTCCACCAAGGGCCCATCGGTCTTCCCCCTGGCACCCTCCTCCAAGAGCACCTCTGGGGGCACAGCGGCCCTGGGC',\n",
    "        'IGHG2':'GCCTCCACCAAGGGCCCATCGGTCTTCCCCCTGGCGCCCTGCTCCAGGAGCACCTCCGAGAGCACAGCGGCCCTGGGC',\n",
    "        'IGHG3':'GCTTCCACCAAGGGCCCATCGGTCTTCCCCCTGGCGCCCTGCTCCAGGAGCACCTCTGGGGGCACAGCGGCCCTGGGC',\n",
    "        'IGHG4':'GCTTCCACCAAGGGCCCATCCGTCTTCCCCCTGGCGCCCTGCTCCAGGAGCACCTCCGAGAGCACAGCCGCCCTGGGC'}}\n",
    "```\n",
    "\n",
    "The key for the first level of the dictionary is used for searching whether the string pattern exists in the `c_call`, and the second level holds the dictionary for the the reference sequences to align to. The keys in the second level are used for replacing the existing `c_call` annotation if it is returned with the highest alignment score. The function currently only accepts 2-4 reference sequences for the pairwise alignment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfceb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl.pp.assign_isotypes(samples, filename_prefix=\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570ec3a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "    \n",
    "Should you want to use a different reference fasta file for this step, run with the following option:\n",
    "\n",
    "```python\n",
    "ddl.pp.assign_isotypes(samples, blastdb=\"path/to/custom_BCR_constant.fasta\")\n",
    "```\n",
    "\n",
    "The default option will return a summary plot that can be disabled with `plot=False`.\n",
    "</div>\n",
    "    \n",
    "Finally, it's worthwhile to manually check the the sequences for constant calls returned as IGHA1-2, IGHG1-4 and the light chains and manually correct them if necessary.\n",
    "\n",
    "## Step 5: Quantify mutations *(optional)*.\n",
    "\n",
    "At this stage, with all the necessary columns in the files, you can quantify the basic mutational load with `pp.quantify_mutations`, a wrapper of `SHaZaM`'s basic mutational analysis in R [[Gupta2015]](https://academic.oup.com/bioinformatics/article/31/20/3356/195677), before subsequent analyses. This will be covered again later in the `Calculating diversity and mutation` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fe10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# quantify mutations\n",
    "for s in tqdm(samples, desc=\"Basic mutational load analysis \"):\n",
    "    filePath = s + \"/dandelion/filtered_contig_dandelion.tsv\"\n",
    "    ddl.pp.quantify_mutations(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae01466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandelion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
