{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# V(D)J Trajectory\n",
    "\n",
    "This notebook is an identical notebook as per the quickstart tutorial with some additional information in the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dandelion as ddl\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This notebook makes use of [Milopy](https://github.com/emdann/milopy) [[Dann2022]](https://doi.org/10.1038/s41587-021-01033-z) and [Palantir](https://github.com/dpeerlab/Palantir) [[Setty2019]](https://doi.org/10.1038/s41587-019-0068-4), two packages that are not formally Dandelion's dependencies. V(D)J feature space applications are open-ended, this is just one of them. Be sure to install the packages beforehand if you want to follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import milopy.core as milo\n",
    "import palantir\n",
    "\n",
    "#required because of Palantir\n",
    "%matplotlib inline\n",
    "\n",
    "sc.settings.set_figure_params(dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We've prepared a demo object based on the TCR trajectory shown in the manuscript for you to use here. It's had some analysis done on the GEX, and has Dandelion-derived contig information merged into it. You can download it from the ftp site as per below or from this [demo repo](https://github.com/zktuong/dandelion-demo-files).\n",
    "\n",
    "It's possible to use V(D)J information that comes from other sources than Dandelion processing, e.g. the pseudobulking will work with Scirpy output. The functions are just calibrated to work with Dandelion's structure by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"demo-pseudobulk.h5ad\"):\n",
    "    os.system(\"wget ftp://ftp.sanger.ac.uk/pub/users/kp9/demo-pseudobulk.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Prior to performing the pseudobulking, it is recommended to run `ddl.tl.setup_vdj_pseudobulk()`. This will subset the object to just cells with paired chains, and prepare appropriately named and formatted columns for the pseudobulking function to use as defaults.\n",
    "\n",
    "If working with non-Dandelion V(D)J processing, subset your cells to ones with at least a full pair of chains, and ensure that you have four columns in place which contain the V(D)J calls for both of the identified primary chains. Scirpy stores this information natively.\n",
    "\n",
    "If you are wanting to include D calls (disabled by default), the recommendation is to subset to only cells/contigs with d_call annotated otherwise the separation could be unreliable (due to missing d_call because of technical reasons rather than biology).\n",
    "\n",
    "Please look at the options for `ddl.tl.setup_vdj_pseudobulk()` carefully to tailor to your use case.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Different setup options/examples\n",
    "    \n",
    "Example #1: if you are wanting to use ALL contigs, regardless of whether they are productive, you would to toggle:\n",
    "<br>\n",
    "```python\n",
    "ddl.tl.setup_vdj_pseudobulk(adata, productive_vdj = False, productive_vj = False)\n",
    "```\n",
    "<br>\n",
    "    \n",
    "Example #2: if you are wanting to use just productive J chains (i.e. you don't mind that V gene is not annotated or not productive) you would to toggle:\n",
    "<br>\n",
    "```python\n",
    "ddl.tl.setup_vdj_pseudobulk(adata, \n",
    "    productive_vdj = False,\n",
    "    productive_vj = True,\n",
    "    check_vdj_mapping = None,\n",
    "    check_vj_mapping = [\"j_call\"])\n",
    "```\n",
    "<br>\n",
    "\n",
    "Example #3: if you are wanting to do it only BCR, gdTCR, abTCR(default), toggle the `mode` option:\n",
    "<br>\n",
    "```python\n",
    "ddl.tl.setup_vdj_pseudobulk(adata, mode = 'B')\n",
    "ddl.tl.setup_vdj_pseudobulk(adata, mode = 'gdT')\n",
    "ddl.tl.setup_vdj_pseudobulk(adata, mode = 'abT')\n",
    "```\n",
    "<br>\n",
    "\n",
    "Example #4: if you want to customise the input (e.g. ignore default columns and using both abT and gdT), you can adjust to the following:\n",
    "<br>\n",
    "```python\n",
    "ddl.tl.setup_vdj_pseudobulk(adata,\n",
    "    mode = None,\n",
    "    extract_cols = ['v_call_VDJ', 'd_call_VDJ', 'j_vall_VDJ', 'v_call_VJ', 'j_call_VJ']\n",
    "    check_extract_cols_mapping = ['v_call_VDJ', 'd_call_VDJ', 'j_vall_VDJ', 'v_call_VJ', 'j_call_VJ']) # this should be identical to extract_cols\n",
    "``` \n",
    "<br>\n",
    "specifying `check_vdj_mapping`, `check_vj_mapping`, and/or `check_extract_cols_mapping` as `None` means that all cells will be used (including those without immune receptors; not advised).\n",
    "    \n",
    "</div>\n",
    "\n",
    "We will proceed with the default settings, which is to only consider the primary V and J calls (productive and highest UMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"demo-pseudobulk.h5ad\")\n",
    "adata = ddl.tl.setup_vdj_pseudobulk(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We're going to be using Milopy to create pseudobulks. Construct a neighbour graph with many neighbours, following Milopy protocol, and then sample representative neighbourhoods from the object. This saves a cell-by-pseudobulk matrix into `adata.obsm[\"nhoods\"]`. Use this graph to generate a UMAP as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep=\"X_scvi\", n_neighbors=50)\n",
    "milo.make_nhoods(adata)\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now we are armed with everything we need to construct the V(D)J feature space. Pseudobulks can be defined either via passing a list of `.obs` metadata columns, the unique values of the combination of which will serve as individual pseudobulks (via `obs_to_bulk`), or via an explicit cell-by-pseudobulk matrix (via `pbs`). Milopy created one of those for us, so we can use that as input.\n",
    "\n",
    "The cell type annotation lives in `.obs[\"anno_lvl_2_final_clean\"]`. Let's tell the function that we want to take the most common value per pseudobulk with us to the new V(D)J feature space object.\n",
    "\n",
    "For non-Dandelion V(D)J processing, use the `cols` argument to specify which `.obs` columns contain the V(D)J calls for the identified primary chains. For Scirpy, this would mean specifying e.g. `cols = ['IR_VDJ_1_v_gene', 'IR_VDJ_1_j_gene', 'IR_VJ_1_v_gene', 'IR_VJ_1_j_gene']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_adata = ddl.tl.vdj_pseudobulk(\n",
    "    adata, pbs=adata.obsm[\"nhoods\"], obs_to_take=\"anno_lvl_2_final_clean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "There is a similar function `ddl.tl.pseudobulk_gex` that takes similar options but pseudobulks the gene expression data instead of making the VDJ feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The new object has pseudobulks as observations, and the unique encountered V(D)J genes as the features. We can see the per-pseudobulk annotation, and `.uns[\"pseudobulk_assigments\"]`. In our case it's just a copy of the `pbs` argument, but if we were to go for `obs_to_bulk` this would be a cells by pseudobulks matrix capturing the assignment of the original cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Now that we have our V(D)J feature space pseudobulk object, we can do things with it. Let's run a PCA on it. The development trajectory is very nicely captured in the first two PC dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.pca(pb_adata)\n",
    "sc.pl.pca(pb_adata, color=\"anno_lvl_2_final_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Let's define the start of our trajectory as the right-most cell, the CD4 terminal state as the bottom-most cell, and the CD8 terminal state as the top-most cell. We can then follow Palantir protocol to generate a diffusion map and run pseudotime. Once done, we rename the terminal states to be more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootcell = np.argmax(pb_adata.obsm[\"X_pca\"][:, 0])\n",
    "terminal_states = pd.Series(\n",
    "    [\"CD8+T\", \"CD4+T\"],\n",
    "    index=pb_adata.obs_names[\n",
    "        [\n",
    "            np.argmax(pb_adata.obsm[\"X_pca\"][:, 1]),\n",
    "            np.argmin(pb_adata.obsm[\"X_pca\"][:, 1]),\n",
    "        ]\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run diffusion maps\n",
    "pca_projections = pd.DataFrame(pb_adata.obsm[\"X_pca\"], index=pb_adata.obs_names)\n",
    "dm_res = palantir.utils.run_diffusion_maps(pca_projections, n_components=5)\n",
    "ms_data = palantir.utils.determine_multiscale_space(dm_res)\n",
    "ms_data.index = ms_data.index.astype(str)\n",
    "\n",
    "pr_res = palantir.core.run_palantir(\n",
    "    ms_data,\n",
    "    pb_adata.obs_names[rootcell],\n",
    "    num_waypoints=500,\n",
    "    terminal_states=terminal_states.index,\n",
    ")\n",
    "\n",
    "pr_res.branch_probs.columns = terminal_states[pr_res.branch_probs.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We can easily transfer the inferred pseudotime and branching probabilities to the pseudobulk object with the aid of a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_adata = ddl.tl.pseudotime_transfer(pb_adata, pr_res)\n",
    "sc.pl.pca(\n",
    "    pb_adata,\n",
    "    color=[\"pseudotime\", \"prob_CD4+T\", \"prob_CD8+T\"],\n",
    "    color_map=\"coolwarm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We can project back our findings to the original cell space object via another helper function. This will remove any cells not in any of the pseudobulks. In the event of a cell belonging to multiple pseudobulks, the cell's pseudotime will be the average of the pseudobulks weighted by the inverse of the pseudobulk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdata = ddl.tl.project_pseudotime_to_cell(\n",
    "    adata, pb_adata, terminal_states.values\n",
    ")\n",
    "sc.pl.umap(bdata, color=[\"anno_lvl_2_final_clean\"])\n",
    "sc.pl.umap(\n",
    "    bdata,\n",
    "    color=[\"pseudotime\", \"prob_CD4+T\", \"prob_CD8+T\"],\n",
    "    color_map=\"coolwarm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
